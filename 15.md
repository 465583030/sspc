### 使用避免重复爬取

重复爬取可以分为两种个角度去解释
* 爬取重复的内容：互联网中，重复的内容比比皆是，针对重复内容的爬取无可避免。
* 爬取重复的URL：重复URL的爬取是软件工程范畴的问题，我们通过技术手段尽心给解决。比如，在爬取一个URL之前，我们先判断这个Url是否爬取过，如果已经爬取过就可以跳过不再爬取。

针对重复URL的解决，我们可以使用以下几种技术方案：
* 如果不考虑多线程下的线程安全，可以使用JDK自带的SET集合，SET非空去重的特点恰好能够满足。
* 如果考虑多线程下的线程安全，可以使用JDK自带的concurrent并发包下的concurrentHashMap来解决这个问题。在使用concurrentHashMap时，只是用field字段就好了。
* 如果是分布式爬虫的情况下，可以使用Redis这种自带Set数据结构的内存数据库。
* 【高级】如果想解决Set占用空间的问题，我们可以基于redis编写BloomFilter过滤器使用。
……

感谢阅读传智播客大数据团队整理的搜索爬虫内容。

[改变中国IT教育,我们正在行动](http://www.itcast.cn)

<a href="http://www.itcast.cn/subject/cloudzly/index.shtml?cloud">
<img src="img/bd.png" width="500" style="border:1px solid red;"/>
</a>

