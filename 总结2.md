###虎嗅爬虫开发总结
功能点：
* 爬取首页的所有新闻地址
 只要新闻中包含aritcle关键词，就当做是单个新闻的地址
```java
Elements els = document.select("a");
		for (Element element : els) {
			// 通过分析数据发现 带article的url就是文章
			if (element.attr("href").contains("article")) {
				// 通过分析数据，发现带http字样的不是我们想要的
				if (!element.attr("href").contains("http")) {
					int index = element.attr("href").indexOf("html");
					String article = "https://www.huxiu.com" + element.attr("href").substring(0, index + 4);
					try {
						System.out.println(article);
						urls.put(article);
					} catch (InterruptedException e) {
						// TODO Auto-generated catch block
						e.printStackTrace();
					}
				}
			}
		}
```
* 解析所有的新闻详情页，得到新闻的正文和新闻页面中的其他URL
	* 保存正文：使用spring jdbctemplate(drivermanagerdatasource)
	* 保存URL：发现集合在迭代的过程中，并不能进行更新操作。使用队列的方式保存URL。不再使用for循环来依次解析URL，创建了线程来接。

* 分页功能获取
	* 先获取第二页的内容，在做分页的时候需要三个参数，hash_code\page\dataline。三个参数中hashcode是固定，page是可知的，只有dateline是变动的，变动的规律是上一个请求的返回值。第二页的dateline是首页html自带的。
	* 通过for循环来自动分页
```java
for (int pageInt = 3; pageInt < totalPageNum.intValue(); pageInt++) {
			System.out.println(pageInt+"-----------------------------------------");
			HttpPost hp = getHttpPost(dateLine, pageInt + "", hashCode);
			HashMap<String, Object> res = getHuxiuIndexPostDocument(hp);
			// parser article url
			String hseg = (String) result.get("data");
			Document sd = Jsoup.parse(hseg);
			// 解析
			indexParser.parser(sd, waitingUrls);
			dateLine = (String) result.get("last_dateline");
		}
```

技术点：
* HttpClient：用来请求网页
* Jsoup：用来解析网页
* Queqe:消息队列，典型的生产者与消费者模式，一边生产数据一边消费数据。
* 多线程：创建线程的三种方式（预告线程池）
* 操作时间：simpledataformat
* 操作Json串：Gson
* 操作数据库：spring jdbctemplate，spring datesource

能用，当我们要爬取的数据量很大的，分布式爬虫发展。

