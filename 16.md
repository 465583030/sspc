### 新闻爬虫实战

爬取思路
* 先获取首页所有的新闻Urls
	* 开启线程后台消费Url并保存数据
* 然后进行分页，得到分页的Urls
	* 后台线程继续运行

```java
	static ConcurrentHashMap<String, Object> map = new ConcurrentHashMap<String, Object>();
	private static ArrayBlockingQueue<String> waitingUrls = new ArrayBlockingQueue<String>(1000);
	public static void start(String url, IndexParser indexParser, DetailParser detailParser,
			final Save2DB save2DB) throws Exception {
		
		System.out.println("index ");
		// ---------------------------------解析首页--------------------
		// 1、准备首页的httpGet对象
		HttpGet indexHttpGet = getHuXiuHttpGet(url);
		// 2、获取首页的indexDocument
		Document indexDocument = getDocumentByHttpClient(url, indexHttpGet);
		 // 3、获取首页的新闻urls
		 indexParser.parser(indexDocument,waitingUrls);
		 // 4、解析URL的信息
		 new ConsumerQueue(waitingUrls,detailParser,map).start();
		 new ConsumerQueue(waitingUrls,detailParser,map).start();
		 new ConsumerQueue(waitingUrls,detailParser,map).start();
		 new ConsumerQueue(waitingUrls,detailParser,map).start();
		 new ConsumerQueue(waitingUrls,detailParser,map).start();
		 new ConsumerQueue(waitingUrls,detailParser,map).start();

		// ------------------------------------分页操作---------------------
		// 5、分页
		// 请求第二页
		 System.out.println("second");
		Elements lastDateLine = indexDocument.select("div[class=get-mod-more js-get-mod-more-list transition]");
		String dateLine = lastDateLine.attr("data-last_dateline");
		String page = "2";
		String hashCode = "392df7cf3fa30d77dd659b4506472ab0";
		// 封装参数
		HttpPost httpPost = getHttpPost(dateLine, page, hashCode);
		// 请求
		HashMap<String, Object> result = getHuxiuIndexPostDocument(httpPost);
		String htmlSegment = (String) result.get("data");
		System.out.println(htmlSegment);
		Document segmentDocument = Jsoup.parse(htmlSegment);
		indexParser.parser(segmentDocument, waitingUrls);
		
		
		System.out.println("second parser success!");
		//6、开始分页请求
		Double totalPageNum = (Double)result.get("total_page");
		dateLine = (String) result.get("last_dateline");
		System.out.println(totalPageNum);
		for (int pageInt = 3; pageInt < totalPageNum.intValue(); pageInt++) {
			System.out.println(pageInt+"-----------------------------------------");
			HttpPost hp = getHttpPost(dateLine, pageInt + "", hashCode);
			HashMap<String, Object> res = getHuxiuIndexPostDocument(hp);
			// parser article url
			String hseg = (String) result.get("data");
			Document sd = Jsoup.parse(hseg);
			// 解析
			indexParser.parser(sd, waitingUrls);
			dateLine = (String) result.get("last_dateline");
		}

	}

```