## 网络爬虫攻防策略分析

海阔天空

### 1、验证码识别

所谓道高一次魔高一丈，验证码早已从最初的数字变成现在的丧心病狂，不再单纯。

好在我们做爬虫，主要爬取一些公开的信息，遇到破解验证码的情况还是较少。

如果不幸遇到需要验证码的网站，建议直接放弃。

如果实在无法放弃，有两种尝试：
* 请先手动登录并拷贝登录信息
* 请购买商业产品。

更多关于[验证码](https://www.zhihu.com/question/35085930)的故事。

### 2、一些常见的限制方式[来源](https://piaosanlang.gitbooks.io/spiders/content/10day/section10.3.html)

* Basic Auth 一般会有用户授权的限制，会在headers的Autheration字段里要求加入；
* Referer 通常是在访问链接时，必须要带上Referer字段，服务器会进行验证，例如抓取京东的评论；

* User-Agent 会要求真是的设备，如果不加会用编程语言包里自有User-Agent，可以被辨别出来；
* Cookie 一般在用户登录或者某些操作后，服务端会在返回包中包含Cookie信息要求浏览器设置Cookie，没有Cookie会很容易被辨别出来是伪造请求；
也有本地通过JS，根据服务端返回的某个信息进行处理生成的加密信息，设置在Cookie里面；
* Gzip 请求headers里面带了gzip，返回有时候会是gzip压缩，需要解压；
* JavaScript加密操作 一般都是在请求的数据包内容里面会包含一些被javascript进行加密限制的信息，例如新浪微博会进行SHA1和RSA加密，之前是两次SHA1加密，然后发送的密码和用户名都会被加密；
* 其他字段 因为http的headers可以自定义地段，所以第三方可能会加入了一些自定义的字段名称或者字段值，这也是需要注意的。
* 真实的请求过程中，其实不止上面某一种限制，可能是几种限制组合在一次，比如如果是类似RSA加密的话，可能先请求服务器得到Cookie，然后再带着Cookie去请求服务器拿到公钥，然后再用js进行加密，再发送数据到服务器。

所以弄清楚这其中的原理，并且耐心分析很重要。

### 3、防封禁策略[来源](https://doc.scrapy.org/en/master/topics/practices.html#avoiding-getting-banned)
* 动态设置user agent
* 禁用cookies（见COOKIES_ENABLED），因为某些网站可能会使用Cookie来查找漫游器的行为
* 使用下载延迟
* 如果可能，请使用Google缓存来抓取网页，而不是直接点击网站
* 使用一个旋转IP池。
* 使用高度分布的下载程序来规避内部禁止

### 4、快速分布式
* 将数据库地址配置到统一的服务器上
* 数据库设置仅允许特定IP老远的访问请求
	* 设置防火墙
	

### 参考资料
* [如何应对网站反爬虫策略？如何高效地爬大量数据?](https://www.zhihu.com/question/28168585)
	